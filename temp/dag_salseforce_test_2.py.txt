import logging
import pandas as pd
import pendulum # datetime 대신 pendulum 사용 권장

from airflow.decorators import dag, task
from airflow.providers.salesforce.hooks.salesforce import SalesforceHook
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.models.baseoperator import BaseOperator # 필요시 BaseOperator 상속

# --- 상수 정의 ---
SALESFORCE_CONN_ID = "Salesforce_Conn" # 실제 Airflow Connection ID 확인 필요
S3_CONN_ID = "my_s3_conn" # S3/MinIO Connection ID
S3_BUCKET_NAME = "sfquickstarts"
S3_KEY_PREFIX = "salesforce_data"
SALESFORCE_QUERY = "SELECT Id, Name FROM Account" # 필요시 필드 추가
JSON_ORIENT = "records"
JSON_LINES = True
FORCE_ASCII = False

# --- TaskFlow API 사용 권장 ---
@dag(
    dag_id='dag_salesforce_to_s3_python_improved',
    start_date=pendulum.datetime(2025, 4, 17, tz="UTC"),
    schedule=None,
    catchup=False,
    tags=["prd", "migration", "salesforce", "s3", "python"],
    default_args={
        'owner': 'jonghyun kim',
        'retries': 1,
        'retry_delay': pendulum.duration(minutes=2),
    },
    doc_md="""
    ### Salesforce to S3 DAG (Python Function)

    Salesforce에서 Account 데이터를 조회하여 S3 버킷에 JSON 파일로 저장합니다. (PythonOperator 사용, 로컬 임시 파일 없음)
    - **Salesforce Connection**: `{SALESFORCE_CONN_ID}`
    - **S3 Connection**: `{S3_CONN_ID}`
    - **S3 Bucket**: `{S3_BUCKET_NAME}`
    - **S3 Key Pattern**: `{S3_KEY_PREFIX}/account_data_{{ ds }}_part_*.json.gz`
    """
)

def salesforce_to_s3_python_dag():
  
  @task
  def export_salesforce_to_s3_task(ds=None, **context):
    """
    Salesforce 데이터를 조회하여 S3에 청크 단위로 업로드합니다.
    """
    salesforce_hook = SalesforceHook(salesforce_conn_id=SALESFORCE_CONN_ID)
    s3_hook = S3Hook(aws_conn_id=S3_CONN_ID)
    logging.info("Salesforce 및 S3 Hook 생성 완료.")    
    
    # S3 저장 옵션 설정 (Hook에서 endpoint_url 등 가져오기)
    s3_storage_options = {}
    try:
        # S3Hook에서 connection 정보 가져오기 시도 (Airflow 버전에 따라 다를 수 있음)
        s3_conn = s3_hook.get_connection(S3_CONN_ID)
        if s3_conn.extra_dejson.get('endpoint_url'):
              s3_storage_options['endpoint_url'] = s3_conn.extra_dejson['endpoint_url']
        # Pandas는 key/secret 또는 profile을 사용하므로, hook의 credential 직접 사용
        credentials = s3_hook.get_credentials()
        s3_storage_options['key'] = credentials.access_key
        s3_storage_options['secret'] = credentials.secret_key
        logging.info(f"S3 Storage Options 설정 완료 (Endpoint: {s3_storage_options.get('endpoint_url')})")
    except Exception as e:
        logging.warning(f"S3 Connection에서 추가 옵션(endpoint_url) 로드 중 오류: {e}. 기본 AWS 설정 사용 시도.")
        # endpoint_url이 필수인 경우 여기서 오류 발생시켜야 함
    
    next_records_url = None
    chunk_index = 0
    total_records_processed = 0
    
    try:
      while True:
        if next_records_url:
            # 다음 페이지 조회 (hook.run은 REST API 엔드포인트를 직접 호출)
            response = salesforce_hook.run(endpoint=next_records_url, data=None, headers=salesforce_hook._get_auth_header())
            logging.info(f"Salesforce 다음 페이지 조회 완료 (URL: {next_records_url})")
        else:
            # 첫 페이지 조회
            response = salesforce_hook.make_query(SALESFORCE_QUERY)
            logging.info(f"Salesforce 첫 페이지 조회 완료 (Query: {SALESFORCE_QUERY})")
            
        records = response.get("records")
        
        if not records:
          logging.info("더 이상 처리할 Salesforce 레코드가 없습니다.")
          break
        
        chunk_df = pd.DataFrame(records)
        num_records_in_chunk = len(chunk_df)
        total_records_processed += num_records_in_chunk
        logging.info(f"청크 {chunk_index}: {num_records_in_chunk}개 레코드 처리 (총 {total_records_processed}개)")

        # S3 파일 경로 설정 (논리적 날짜와 청크 인덱스 사용)
        s3_key = f"{S3_KEY_PREFIX}/account_data_{ds}_part_{chunk_index:04d}.json.gz"
        s3_uri = f"s3://{S3_BUCKET_NAME}/{s3_key}"
        
        # 데이터 저장 (압축 포함)
        logging.info(f"청크 {chunk_index} 데이터를 S3에 저장 시작: {s3_uri}")
        chunk_df.to_json(
            s3_uri,
            orient=JSON_ORIENT,
            lines=JSON_LINES,
            force_ascii=FORCE_ASCII,
            compression='gzip',
            storage_options=s3_storage_options
        )
        logging.info(f"청크 {chunk_index} 데이터 S3 저장 완료: {s3_uri}")

        chunk_index += 1
        
        # 다음 페이지 URL 확인
        if response.get("done"):
            logging.info("Salesforce 데이터 처리 완료 (마지막 페이지).")
            break
          
        next_records_url = response.get("nextRecordsUrl")
        
        if not next_records_url:
            logging.warning("Salesforce 응답에 'nextRecordsUrl'이 없습니다. 루프를 종료합니다.")
            break
        
        # nextRecordsUrl은 상대 경로일 수 있으므로, hook의 instance_url과 조합 필요
        if not next_records_url.startswith('http'):
            instance_url = salesforce_hook.conn.instance_url.rstrip('/')
            next_records_url = f"{instance_url}{next_records_url}"
    
    except Exception as e:
        logging.error(f"Salesforce 데이터 처리 중 오류 발생: {e}")
        raise 
                 
  
  export_salesforce_to_s3_task()

# DAG 인스턴스 생성
salesforce_to_s3_python_dag()


# def fn_salesforce_export():
#   salesforce_hook = SalesforceHook(salesforce_conn_id="Salseforce_Conn")
  
#   base_query = "SELECT Id, Name FROM Account"
  
#   endpointUrl = "http://192.168.10.29:9000"
#   awsAccessKeyId = "NkCNrjizEMHIvFrGa3cl"
#   awsSecretAccessKey = "etgLxl58LgPKSWELhDpvrrjdK4VrfoCBFXbK2dtR"
#   orient="records"
  
#   chunk_size = 2  # 청크 크기 설정   최대 오프셋은 2,000. 2,000보다 큰 오프셋을 요청하면유효 범위를 벗어난 숫자오류. 
#   offset = 0
#   coerceToTimestamp = False
#   recordTimeAdded = False
  
#   try:
#     while True:
#       query = f"{base_query} LIMIT {chunk_size} OFFSET {offset}"
#       logging.info("query: " + query)
#       response = salesforce_hook.make_query(query)
#       if not response["records"]:
#         break
      
#       chunk_df = salesforce_hook.object_to_df(
#          query_results=response["records"],
#          coerce_to_timestamp=coerceToTimestamp,
#          record_time_added=recordTimeAdded,
#       )
      
#       logging.info("content: " + chunk_df)

#       # 파일 경로 설정
#       file_path= f"s3://sfquickstarts/salesforce_data/account_data_{datetime.now().strftime('%Y%m%d%H%M%S')}_{offset//chunk_size}.json.gz"
      
#       # 데이터 저장
#       chunk_df.to_json(file_path, orient=orient, lines=True, force_ascii=False,
#         storage_options={'endpoint_url':endpointUrl, 'key':awsAccessKeyId,'secret':awsSecretAccessKey})
      
#       logging.info(f"{file_path} 파일이 적재 돼었습니다..")
      
#       offset += chunk_size
#       logging.info("offset: " + str(offset))
    
#   except Exception as e:
#     logging.error("Error message : %s", e)
#     raise  

# default_args = {
#     'owner': 'jonghyun kim',
#     'retries': 1,
#     'retry_delay': timedelta(minutes=2)
# }

# with DAG(
#     dag_id='dag_salesforce_test_2',
#     default_args=default_args,
#     description='Salesforce 데이터 처리 예제',
#     start_date=datetime(2025, 3, 27),
#     schedule_interval=None,
#     catchup=False,
#     tags=["prd", "migration"],
# ) as dag:
#       salesforce_export_task = PythonOperator(
#       task_id='salesforce_export_task',
#       python_callable=fn_salesforce_export,
# )

# salesforce_export_task  

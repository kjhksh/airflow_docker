import gc
import logging
import yaml
import os
import re
import shutil
import json
import ast
import numpy as np
import pandas as pd
import s3fs
import boto3

from datetime import datetime, timedelta
from tempfile import NamedTemporaryFile
from pathlib import Path
import xml.etree.ElementTree as ET
from concurrent.futures import ThreadPoolExecutor

from airflow import DAG
from airflow.operators.empty import EmptyOperator
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator

from airflow.providers.oracle.hooks.oracle import OracleHook
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.providers.amazon.aws.hooks.s3 import S3Hook


# config 파일 불러오기
conf_url = Path(__file__).parent / "resource/dag_with_rdbms_tar_ETL_config.yaml"
with open(conf_url, 'r') as f:
	conf = yaml.load(f, Loader=yaml.FullLoader)

logging.info("load config file  : %s ", conf_url)
logging.info(f"project name : {conf['name']}")
logging.info(f"project version : {conf['version']}")
logging.info(f"project description : {conf['description']}")
logging.info(f"project tags : {conf['tags']}")
logging.info(f"project start : {datetime.today().strftime('%Y-%m-%d %H:%M:%S')}")

# sql 파일 불러오기
sql_url = Path(__file__).parent / conf['jobs']['settings']['sources']['sql_path']

# MinIO 클라이언트 생성
# s3 = boto3.client('s3', endpoint_url='http://192.168.10.29:9000', aws_access_key_id='NkCNrjizEMHIvFrGa3cl', aws_secret_access_key='etgLxl58LgPKSWELhDpvrrjdK4VrfoCBFXbK2dtR')
# s3 = boto3.client('s3', endpoint_url='http://192.168.10.29:9000', aws_access_key_id='admin', aws_secret_access_key='cspi00P@ssWord')
# print('craete connection:', s3)


def clean_space():
    logging.info("# step 4: Clean up resources and global variables")
    
    try:
        global conf_url, conf, sql_url
        del conf_url, conf, sql_url

        # 초기화
        for obj in list(globals().keys()):
            # checking for built-in variables/functions
            # print(globals().keys())
            if not obj.startswith('_'):
                # deleting the said obj, since a user-defined function
                del globals()[obj]
    except:
        gc.collect()
        

def change_exp_chars(text, ref_dict=None):
    exp_list = re.compile('\{([^}]+)').findall(text)
    for exp in exp_list:
        text = text.replace('{'+exp+'}', str(ref_dict[exp])).strip()
    return text

def get_query_from_xml(file_path, query_id):
    tree = ET.parse(file_path)
    root = tree.getroot()
    for query in root.findall('query'):
        if query.get('id') == query_id:
            return query.text.strip()
    return None

def execute_query(hook, job, num_rows_per_file, file_format):
    
    query = get_query_from_xml(sql_url, job['id'])
    # queryparams={'id': 'admin'}
    queryparams = ast.literal_eval("{" + job['queryparams'] + "}")
    
    #extract
    try:
        conn = hook.get_conn()
        cursor = conn.cursor()
        data_tf = False
        
        bucket_name =  conf['dbs']['bucket_name']
        file_path = job['targets']['file_path']
        account_name = job['targets']['account_name']
        table_name = job['targets']['table_name']
        
        file_path = change_exp_chars(file_path, ref_dict={'bucket_name':bucket_name,
                                                            'account_name':account_name,
                                                            'table_name':table_name,
                                                            'file_format':file_format})
        
        square_exp_list = re.compile('\[([^]]+)').findall(file_path)
        
        #[숫자] 형태가 있으면 start_num과 step을 만든다.
        for square_exp in square_exp_list:
            if re.search('^[0-9]', square_exp):
                num = int(square_exp.split(',')[0])
                step = int(square_exp.split(',')[1])
        
        # 디렉토리 생성
        dir_path = Path(file_path).parent
        if not dir_path.exists():
            dir_path.mkdir(parents=True, exist_ok=True)
            logging.info(f"{dir_path} 디렉토리가 생성되었습니다.")
        
         # 로그 정보를 저장할 딕셔너리 초기화
        log_info_list = []
        
        for chunk_df in pd.read_sql(query, conn, params=queryparams, chunksize=num_rows_per_file):
        # for chunk_df in pd.read_sql(query, conn, chunksize=num_rows_per_file):
            if len(chunk_df) > 0:
                data_tf = True
                for square_exp in square_exp_list:
                    if re.search(r'^[0-9]', square_exp):    
                        final_file_path = file_path.replace('['+square_exp+']', str(num)).strip()  
                        
                        logging.info("final_file_path ::"+final_file_path)
                        # chunk_df.to_json(final_file_path, orient="records", lines=True, force_ascii=False)      
                        
                        # final_file_path = "s3://sfquickstarts/tastybytes/raw_pos/order/order_1.json.gz"
                        # # final_file_path = "tastybytes/raw_pos/order/order_1.json.gz"
                         
                        chunk_df.to_json('s3://sfquickstarts/tastybytes/raw_pos/order/order_1.json.gz', orient="records", lines=True, force_ascii=False,
                                            storage_options={'endpoint_url':'http://192.168.10.29:9000', 'key':'NkCNrjizEMHIvFrGa3cl','secret':'etgLxl58LgPKSWELhDpvrrjdK4VrfoCBFXbK2dtR'})
                        # chunk_df.to_json('s3://sfquickstarts/tastybytes/raw_pos/order/order_1.json.gz', orient="records", lines=True, force_ascii=False,
                        #                     storage_options={'endpoint_url':'http://192.168.10.29:9000', 'key':'admin','secret':'cspi00P@ssWord'})
                        
                        num = num + step     
                        
                        # 데이터 카운트 및 null 카운트 로그 추가
                        log_info = {
                            "file_path": final_file_path,
                            "row_count": len(chunk_df),
                            "null_count": chunk_df.isnull().sum().to_dict()
                        }
                        log_info_list.append(log_info)
                       
        # 로그 정보를 하나의 JSON 파일로 저장
        log_file_path = Path(file_path).parent / (table_name + "_log.json")
        with open(log_file_path, 'w') as log_file:
            json.dump(log_info_list, log_file)
            
        logging.info(f"{log_file_path} 로그가 생성되었습니다.")
            
        if data_tf:
            logging.info(" - load complete.")
        else:
            logging.info("- skip load job. (0 rows)")
        '''
        if data_tf:
            print(f"job_id : {job['id']} job_name : {job['targets'][0]['file_name']} - load complete.")
        else:
            print(f"job_id : {job['id']} job_name : {job['targets'][0]['file_name']} - skip load job. (0 rows)")
        '''                                    
        cursor.close()
        conn.close()
    except Exception as e:
        logging.error("Error message : %s", e)
        raise
                            
# def initialize_data_directory(directory_path):
#     pass
    # if os.path.exists(directory_path):
    #     os.system(f'rm -rf {directory_path}')
    #     logging.info(f"{directory_path} 디렉토리가 초기화되었습니다.")
    # 필요시 추가 S3 초기화
    # s3_hook = S3Hook(aws_conn_id='locknlock_s3')
    # bucket_name = "sfquickstarts"
    # folder_path = "tastybytes"
    # # 폴더 내의 모든 파일 삭제
    # keys_to_delete = s3_hook.list_keys("sfquickstarts", prefix="tastybytes")
    # if keys_to_delete:
    #     for key in keys_to_delete:
    #         logging.info(f"Deleting {key}")
    #         s3_hook.delete_objects(bucket_name, keys=[key])
    #     logging.info(f"All files in {folder_path} have been deleted.")
    # else:
    #     logging.info(f"No files found in {folder_path}.")
    
    # print("keys_to_delete :: "+str(keys_to_delete))
    
def fn_oralceExport(job_list, num_rows_per_file, file_format):
    
    logging.info("# step 1: query data from oralceSql db and save into json.gz file 생성 ")
    logging.info(datetime.today().strftime('%Y-%m-%d %H:%M:%S'))
    
    oracle_conn_id = conf['jobs']['settings']['sources']['name']
    hook = OracleHook(oracle_conn_id=oracle_conn_id)
    
    # for job in job_list:
    #     execute_query(hook, job, num_rows_per_file, file_format)
    
    def worker(job):
        execute_query(hook, job, num_rows_per_file, file_format)
    
    # ThreadPoolExecutor를 사용하여 최대 4개의 쓰레드만 동시에 실행되도록 설정
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(worker, job) for job in job_list]
        
        for future in futures:
            try:
                future.result()  # 각 쓰레드의 결과를 기다림
            except Exception as e:
                logging.error("Error message : %s", e)
        
def fileTos3_task():
    
    logging.info("# step 3: upload  file into S3")
    
    # aws_conn_id = conf['jobs']['settings']['targets']['name']
    # # s3_hook = S3Hook(aws_conn_id=aws_conn_id)
    # s3_hook = S3Hook(aws_conn_id="mini_test")
    
    # folder_path = "./data/"
    # bucket_name =  conf['dbs']['bucket_name']
    # #load
    # try:
    #     for root, dirs, files in os.walk(folder_path):
    #         for file_name in files:
    #             file_path = os.path.join(root, file_name)
    #             s3_key = os.path.join("", os.path.relpath(file_path, folder_path+bucket_name))
                
    #             logging.info("file_path ::"+file_path)
    #             logging.info("s3_key ::"+s3_key)
                
    #             s3_hook.load_file(
    #                 filename=file_path,
    #                 key=str(s3_key),  # 문자열로 변환
    #                 bucket_name=str(bucket_name),  # 문자열로 변환
    #                 replace=True
    #             )
    # except Exception as e:
    #     logging.error("Error message : %s", e)
    #     raise
    
               
default_args = {
    'owner': 'jonghyun kim',
    'retries': 1,
    'retry_delay': timedelta(minutes=2)
}

with DAG(
    dag_id=conf['name'],
    default_args=default_args,
    description = conf['description'],
    start_date = datetime(2025, 3, 27),
    schedule_interval = None,
    catchup = False,
    tags = conf['tags']
) as dag:
    
    targetFolder =  conf['jobs']['settings']['targets']['folder']
    
    initialize_data_task = EmptyOperator(
        task_id='start_task'
    )
    
    
    job_list = conf['jobs']['job_list']
    num_rows_per_file = conf['jobs']['settings']['targets']['num_rows_per_file']
    file_format = conf['jobs']['settings']['targets']['file_format']
    
    oralceExport_task = PythonOperator(
        task_id='oralceExport_task',
        python_callable=fn_oralceExport,
        op_args=[job_list, num_rows_per_file, file_format]
        
    )
    
    aws_conn_id = conf['jobs']['job_list']
    
    fileTos3_task = PythonOperator(
        task_id='fileTos3_task',
        python_callable=fileTos3_task
    
    )

    cleanSpace_task = PythonOperator(
        task_id='cleanSpace_task',
        python_callable=clean_space
    
    )

initialize_data_task >> oralceExport_task  >> fileTos3_task >> cleanSpace_task
